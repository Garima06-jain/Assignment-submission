-- 1. Customer Table
CREATE TABLE Customer (
    CustomerID INT PRIMARY KEY,
    Name VARCHAR(100),
    Address VARCHAR(255),
    Email VARCHAR(100),
    Phone VARCHAR(20)
);

INSERT INTO Customer VALUES
(1, 'Alice Johnson', '12 Hill St', 'alice@mail.com', '9876543210'),
(2, 'Bob Singh', '34 Oak Ave', 'bob@mail.com', '8765432190');

-- 2. Product Table
CREATE TABLE Product (
    ProductID INT PRIMARY KEY,
    Name VARCHAR(100),
    Description VARCHAR(255),
    Price DECIMAL(10,2),
    Category VARCHAR(50)
);

INSERT INTO Product VALUES
(101, 'Laptop', 'Dell Inspiron 15', 60000.00, 'Electronics'),
(102, 'Mobile', 'Samsung Galaxy M31', 20000.00, 'Electronics');

-- 3. Order Table
CREATE TABLE [Order] (
    OrderID INT PRIMARY KEY,
    CustomerID INT FOREIGN KEY REFERENCES Customer(CustomerID),
    ProductID INT FOREIGN KEY REFERENCES Product(ProductID),
    Quantity INT,
    OrderDate DATE,
    TotalAmount DECIMAL(10,2)
);

INSERT INTO [Order] VALUES
(1001, 1, 101, 1, '2025-06-01', 60000.00),
(1002, 2, 102, 2, '2025-06-02', 40000.00);

-- 4. Inventory Table
CREATE TABLE Inventory (
    ProductID INT PRIMARY KEY FOREIGN KEY REFERENCES Product(ProductID),
    Quantity INT,
    Location VARCHAR(100)
);

INSERT INTO Inventory VALUES
(101, 50, 'Delhi Warehouse'),
(102, 70, 'Mumbai Warehouse');


Enabled CDC on SQL Server Tables:

EXEC sys.sp_cdc_enable_db;
EXEC sys.sp_cdc_enable_table @source_schema='dbo', @source_name='Customer', @role_name=NULL;


Created Azure Databricks Notebook:

from pyspark.sql import SparkSession
jdbc_url = "jdbc:sqlserver://<server>.database.windows.net:1433;databaseName=<db>"
properties = {"user": "<username>", "password": "<password>", "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"}

df = spark.read.format("jdbc").option("url", jdbc_url).option("dbtable", "cdc.dbo_Customer_CT") \
     .option("user", properties["user"]).option("password", properties["password"]) \
     .option("driver", properties["driver"]).load()

df.write.mode("overwrite").format("csv").save("/mnt/datalake/cdc/customer")




